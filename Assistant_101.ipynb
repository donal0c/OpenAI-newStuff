{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkj7YmjvQPoxFINs0EXypx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donal0c/HuggingFaceTutorials/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5lUc_WX0ICJ",
        "outputId": "719769c6-ffc7-4cee-8a85-0052ec5a3df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.2.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"sk-**************\")"
      ],
      "metadata": {
        "id": "4u5C6Jw_0Kvk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Math Tutor\",\n",
        "    instructions=\"You are a personal math tutor. Write and run code to answer math questions.\",\n",
        "    tools=[{\"type\": \"code_interpreter\"}],\n",
        "    model=\"gpt-4-1106-preview\"\n",
        ")"
      ],
      "metadata": {
        "id": "l1oK487Q0V7U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "print(thread)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRNRgVSy0YG0",
        "outputId": "d3f5abdf-bfb2-4edd-e92f-ea2da79c3016"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread(id='thread_XcuD4m4Ck6tYh3W7fqE1wPg2', created_at=1699898134, metadata={}, object='thread')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id = thread.id,\n",
        "    role = \"user\",\n",
        "    content = \"Solve this problem: 3x + 11 = 14\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZeNS2Rv90u0E"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3z2Xtsc1MPs",
        "outputId": "ad71dd5f-591f-42bf-9e1b-b35667fc5b7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ThreadMessage(id='msg_JvfVPNmHOqoNHaEzoOz5uYbL', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='Solve this problem: 3x + 11 = 14'), type='text')], created_at=1699898276, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_XcuD4m4Ck6tYh3W7fqE1wPg2')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id = thread.id,\n",
        "    assistant_id = assistant.id\n",
        ")"
      ],
      "metadata": {
        "id": "2PmzBH141V-G"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.retrieve(\n",
        "    thread_id = thread.id,\n",
        "    run_id = run.id\n",
        ")"
      ],
      "metadata": {
        "id": "uGtuRNTFByq5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "    thread_id = thread.id\n",
        ")"
      ],
      "metadata": {
        "id": "vJKyiOvGCLsp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in reversed(messages.data):\n",
        "  print(message.role + \": \" + message.content[0].text.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vlaf6i_ICYhJ",
        "outputId": "11d5357e-6d7e-4060-b9e9-3700d5cf995b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: Solve this problem: 3x + 11 = 14\n",
            "assistant: The solution to the equation \\(3x + 11 = 14\\) is \\(x = 1\\).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = client.files.create(\n",
        "    file=open(\"/content/memgpt-paper.pdf\", \"rb\"),\n",
        "    purpose = \"assistants\"\n",
        ")"
      ],
      "metadata": {
        "id": "kylHZbAQCrWJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyT2bxsWE4tW",
        "outputId": "2478873a-2dd6-4091-caca-570bdc618c6d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FileObject(id='file-AHdLsEmGS3AtxF7JPXAi3HOL', bytes=602501, created_at=1699902360, filename='memgpt-paper.pdf', object='file', purpose='assistants', status='processed', status_details=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Machine Learning researcher\",\n",
        "    instructions=\"You are a machine learning researcher, answer questions on the resuarch paper \",\n",
        "    tools=[{\"type\": \"retrieval\"}],\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    file_ids = ['file-AHdLsEmGS3AtxF7JPXAi3HOL']\n",
        ")"
      ],
      "metadata": {
        "id": "hrOBgNZME9Wn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n",
        "print(thread)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Bj3yYaFaGq",
        "outputId": "c417b7cd-c17d-4cfa-a5f1-4c105f2c080f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thread(id='thread_Ha4SEs4KiUdffHVS0aN8B0UD', created_at=1699902590, metadata={}, object='thread')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id = thread.id,\n",
        "    role = \"user\",\n",
        "    content = \"How does memgpt allow LLMS to have unlimited context length\"\n",
        ")"
      ],
      "metadata": {
        "id": "Kb4VX99OFk7Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "    thread_id = thread.id,\n",
        "    assistant_id = assistant.id\n",
        ")"
      ],
      "metadata": {
        "id": "12vPM2H3GM96"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.retrieve(\n",
        "    thread_id = thread.id,\n",
        "    run_id = run.id\n",
        ")"
      ],
      "metadata": {
        "id": "IOH80F9QGN-q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "    thread_id = thread.id\n",
        ")\n"
      ],
      "metadata": {
        "id": "QuLgpagyGSrq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in reversed(messages.data):\n",
        "  print(message.role + \": \" + message.content[0].text.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IK_OoVQbGVtp",
        "outputId": "3e67776a-ab12-4783-c944-c9cf87537ac1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: How does memgpt allow LLMS to have unlimited context length\n",
            "user: How does memgpt allow LLMS to have unlimited context length\n",
            "user: How does memgpt allow LLMS to have unlimited context length\n",
            "assistant: MemGPT allows LLMS (Large Language Model Systems) to handle what effectively seems like unlimited context length by utilizing several strategies for memory management:\n",
            "\n",
            "1. **Main Context Management**: The main context within MemGPT is divided into three components: system instructions (read-only and pinned), conversational context (read-only with a special eviction policy), and working context (writeable by the LLM processor). These combined cannot exceed the maximum context size of the underlying LLM processor. When the conversational context reaches a certain size, a portion of it is either truncated or compressed via recursive summarization【11†source】.\n",
            "\n",
            "2. **External Context**: MemGPT uses out-of-context storage, referred to as 'external context', which functions similarly to disk storage in an operating system. The information stored in the external context is not immediately visible to the LLM processor but can be accessed and brought into the main context through function calls. This external context comprises recall storage (the full history of events processed by the LLM processor) and archival storage (a general read-write datastore for overflow). This allows MemGPT to handle information beyond the token limit of the main context and perform actions like recalling previous conversations or searching through large document databases【12†source】.\n",
            "\n",
            "3. **Self-directed Editing and Retrieval**: MemGPT can autonomously make decisions about when and what information to move between contexts. It updates and searches through its own memory based on the current context and instructions provided within the preprompt. It also receives warnings when nearing token limitations, and the system can take appropriate actions such as saving important information before memory is trimmed【12†source】.\n",
            "\n",
            "4. **Pagination**: To avoid overflow of the context window during memory retrieval, MemGPT implements pagination for handling external context data. This means that data is fed into the main context in chunks, allowing the system to manage more information than the context window would typically allow【12†source】.\n",
            "\n",
            "Together, these mechanisms allow MemGPT to manage and process context effectively without being limited by the fixed-size context window that is typical of LLM processors, giving the experience of working with an unlimited context length.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ac5zZihvGbK5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
